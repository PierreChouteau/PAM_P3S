{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "QYQ3OATSdv_W"
   },
   "source": [
    "## Simulation d'une situation réel avec Pyroomacoustics"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: \n",
    "\n",
    "Il y a plusieurs possibilités dans ce notebook: \n",
    "\n",
    "- Utilisation de différents algorithmes (Celui de Louis, ou celui de pyroomacoustics)\n",
    "Pour utiliser l'un ou l'autre, il faut faire attention aux variables globales (définies avec des MAJUSCULES). \n",
    "- Utilisation de fichiers MuseDB ou de fichiers WAV.\n",
    "- Possibilité de plot les performances en fonction de l'avancée de l'algorithme. (PLOT_PERFORMANCE) mais ATTENTION, cela est implémenté que pour l'algo pyroom pour l'instant. \n",
    "- Possibilité de sauvegarder les figures et les audios (SAVE_FIG et SAVE_AUDIO), cela va créé les dossiers correspondants sous 'test'\n",
    "\n",
    "- Puis bien sûr, possibilité de modifier toutes les données concernant l'emplacement des microphones, dimensions de la salle etc... au fur et à mesure du notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "from  IPython.display import display\n",
    "import pickle\n",
    "import librosa\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import stempeg\n",
    "import itertools\n",
    "import operator\n",
    "import soundfile\n",
    "import pyroomacoustics as pra\n",
    "import wave\n",
    "import random as rd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "from pyroomacoustics.directivities import (\n",
    "    DirectivityPattern,\n",
    "    DirectionVector,\n",
    "    CardioidFamily,\n",
    ")\n",
    "from numpy import typing\n",
    "from mir_eval.separation import bss_eval_sources\n",
    "\n",
    "path_in = './data/musedb/'\n",
    "save_path = './test/surdeterm/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src import performance\n",
    "from src.fast_nmf import fast_MNMF2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargmenent d'un fichier MuseDB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All files in MUSDB18 dataset is a multitrack format composed of 5 stereo streams, each one encoded in AAC @256kbps. These signals correspond to:\n",
    "\n",
    "- `0` - The mixture,\n",
    "- `1` - The drums,\n",
    "- `2` - The bass,\n",
    "- `3` - The rest of the accompaniment,\n",
    "- `4` - The vocals.\n",
    "\n",
    "```S.shape = (5, time_step , 2)```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_in = []\n",
    "files_title = []\n",
    "\n",
    "# extract files \n",
    "for r, d, f in os.walk(path_in):\n",
    "    for file in f:\n",
    "        if '.mp4' in file:\n",
    "            # file address\n",
    "            files_in.append(os.path.join(r, file))\n",
    "            # file author + song\n",
    "            files_title.append(file[:-9])\n",
    "            \n",
    "files_in.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path in files_in[:3]:\n",
    "    S, rate = stempeg.read_stems(path)\n",
    "    display(ipd.Audio(S[0][:,0], rate=rate))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test avec un fichier wav classique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_files = [['data/samples/BACH Cello Suite 1, Prelude, Violin - Kateryna Timokhina.wav'],\n",
    "            ['data/samples/Bach_ Prélude, Cello suite Nr.1  Ophélie Gaillard.wav'],\n",
    "            ['data/samples/Bach_ Prélude, Cello suite Nr.1  Ophélie Gaillard.wav'],\n",
    "            ['data/samples/BACH Cello Suite 1, Prelude, Violin - Kateryna Timokhina.wav']]\n",
    "\n",
    "signals = [np.concatenate([wavfile.read(f)[1].astype(np.float32) for f in source_files]) for source_files in wav_files]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des variables globales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PLOT_PERFORMANCE = False # plot les performances en fonction des itérations de l'algorithme => Implémenté que pour pyroomacoustics\n",
    "MUSEDB = True # Si True, on utilise la base de données MUSEDB, sinon on utilise les fichiers wav_files\n",
    "LOUIS = True # Si True, on utilise l'algorithme de LOUIS, sinon on utilise l'algorithme de pyroomacoustics\n",
    "SAVE_FIG = True # Si True, on sauvegarde toutes les figures dans les sous-dossiers de 'test'\n",
    "SAVE_AUDIO = True # Si True, on sauvegarde tous les fichiers audio dans le sous-dossiers 'audio' de 'test'\n",
    "SAVE_PERF = True # Si True, on sauvegarde toutes les figures dans les sous-dossiers de 'test'\n",
    "    \n",
    "# Paramètres des microphones\n",
    "mic_pattern = DirectivityPattern.CARDIOID\n",
    "MIC_DIR = CardioidFamily(\n",
    "    orientation=DirectionVector(azimuth=180, colatitude=60, degrees=True),\n",
    "    pattern_enum=mic_pattern,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permet de créé les dossiers de sauvegarde si les variables permettant la sauvegarde\n",
    "# sont à True\n",
    "\n",
    "try:\n",
    "    os.mkdir('./test')\n",
    "    \n",
    "except  OSError as error:\n",
    "    pass\n",
    "\n",
    "\n",
    "if SAVE_FIG:\n",
    "    try:\n",
    "        os.mkdir(save_path)\n",
    "        \n",
    "        try:\n",
    "            os.mkdir(save_path+'activation')\n",
    "            os.mkdir(save_path+'base')\n",
    "            os.mkdir(save_path+'spectro')\n",
    "            os.mkdir(save_path+'mix')\n",
    "        except OSError as error:\n",
    "            pass\n",
    "\n",
    "    except OSError as error:\n",
    "        pass\n",
    "    \n",
    "if SAVE_AUDIO:\n",
    "    try:\n",
    "        os.mkdir(save_path+'audios')\n",
    "        os.mkdir(save_path+'audios/separation')\n",
    "        os.mkdir(save_path+'audios/no_separation')\n",
    "        \n",
    "        os.mkdir(save_path+'audios/separation/micro_drums')\n",
    "        os.mkdir(save_path+'audios/separation/micro_bass')\n",
    "        os.mkdir(save_path+'audios/separation/micro_vocals')\n",
    "        os.mkdir(save_path+'audios/separation/micro_other')\n",
    "        os.mkdir(save_path+'audios/separation/micro_AB1')\n",
    "        os.mkdir(save_path+'audios/separation/micro_AB2')\n",
    "        \n",
    "        os.mkdir(save_path+'audios/no_separation/micro_AB1')\n",
    "        os.mkdir(save_path+'audios/no_separation/micro_AB2')\n",
    "        os.mkdir(save_path+'audios/no_separation/micro_drums')\n",
    "        os.mkdir(save_path+'audios/no_separation/micro_bass')\n",
    "        os.mkdir(save_path+'audios/no_separation/micro_vocals')\n",
    "        os.mkdir(save_path+'audios/no_separation/micro_other')\n",
    "        \n",
    "    except OSError as error:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxrtQ07adbs6"
   },
   "outputs": [],
   "source": [
    "def spectrogram_from_musdb(\n",
    "    room_dimension, \n",
    "    abs_coef, \n",
    "    source_locations,\n",
    "    source_names,\n",
    "    microphone_locations,\n",
    "    microphone_names,\n",
    "    mic_dir,\n",
    "    song_path,\n",
    "    audio_length, \n",
    "    L=2048,\n",
    "    hop=512, \n",
    "    display_audio = False,\n",
    "    display_room = False,\n",
    "    ):\n",
    "\n",
    "    \"\"\" \n",
    "    this function process a song from MUSDB18 dataset\n",
    "    as a recording in a shoebox room (defined with its \n",
    "    geometry, room absorption, signal locations and \n",
    "    microphones locations) into a multichannel STFT.\n",
    "\n",
    "    Inputs:\n",
    "    ------------------------------------------------------\n",
    "    room_dimension: \n",
    "        room dimension (np.array)\n",
    "    abs_coef: \n",
    "        absorption coefficient (int)\n",
    "    source_locations: \n",
    "        localization of the sources in the room (list of list)\n",
    "    source_names: \n",
    "        name of the sources (list of string)\n",
    "    microphone_locations: \n",
    "        localization of the micros in the room (warning, locs in `np.c_` class)\n",
    "    microphone_names: \n",
    "        name of the microphones (list of string)\n",
    "    mic_dir:\n",
    "        microphone directivity (pyroomacoustics directivity object)\n",
    "    song_path: \n",
    "        path to the song (string)\n",
    "    audio_length: \n",
    "        length of the audio signal (int)\n",
    "    L: \n",
    "        frame size (2048 default)\n",
    "    hop: \n",
    "        hop length (512 default)\n",
    "    display_audio: \n",
    "        bool (display audio signal)\n",
    "    display_room: \n",
    "        bool (display room geometry)\n",
    "\n",
    "    \n",
    "    Output:\n",
    "    ------------------------------------------------------\n",
    "    X: (n_frames, n_frequencies, n_channels)\n",
    "    room: pyroomacoustics room object\n",
    "    separate_recordings: (n_sources, n_channels, n_frames)\n",
    "    mics_signals: (n_channels, n_frames)\n",
    "\n",
    "    \"\"\"\n",
    "    path = song_path\n",
    "    data, rate = stempeg.read_stems(path)\n",
    "    channel_nb, time_step, _ = data.shape\n",
    "    X = []\n",
    "\n",
    "    # Create an shoebox room\n",
    "    room = pra.ShoeBox(room_dimension, fs=rate, max_order=15, absorption=abs_coef, sigma2_awgn=1e-8)\n",
    "\n",
    "    # Add sources\n",
    "    for channel_source, source_loc in zip(range(1,len(data)), source_locations):\n",
    "        signal_channel = librosa.core.to_mono(data[channel_source, :rate*audio_length, :].T)\n",
    "        signal_channel /= np.max(signal_channel)\n",
    "        room.add_source(source_loc, signal=signal_channel)\n",
    "\n",
    "    # # Add microphone array\n",
    "    mic_array = pra.MicrophoneArray(microphone_locations, rate)\n",
    "    # # Appoint => orienté dans la direction de l'instrument => cardioide pour l'instant\n",
    "    # # Couple => ORTF 17cm, 110°, 90° angle utile\n",
    "    room.add_microphone_array(mic_array, directivity=mic_dir)\n",
    "    \n",
    "    \n",
    "    if display_room:\n",
    "        fig, ax = room.plot()\n",
    "        lim = np.max(room_dimension)\n",
    "        ax.set_xlim([0, lim])\n",
    "        ax.set_ylim([0, lim])\n",
    "        ax.set_zlim([0, lim])\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "    \n",
    "    # Recordings\n",
    "    separate_recordings = room.simulate(return_premix=True)\n",
    "    mics_signals = np.sum(separate_recordings, axis=0)\n",
    "    \n",
    "    # STFT parameters\n",
    "    win_a = pra.hamming(L)\n",
    "    win_s = pra.transform.stft.compute_synthesis_window(win_a, hop)\n",
    "          \n",
    "    # Observation vector in the STFT domain\n",
    "    X = pra.transform.stft.analysis(mics_signals.T, L, hop, win=win_a)\n",
    "\n",
    "    if display_audio:\n",
    "        for microphone_n in range(microphone_locations.shape[1]) :\n",
    "            print(f\"Microphone {microphone_names[microphone_n]}\")\n",
    "            display(ipd.Audio(mics_signals[microphone_n], rate=room.fs))\n",
    "\n",
    "    return X, room, separate_recordings, mics_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spectrogram_from_wav(\n",
    "    room_dimension, \n",
    "    abs_coef, \n",
    "    source_locations,\n",
    "    source_names,\n",
    "    microphone_locations,\n",
    "    microphone_names,\n",
    "    mic_dir,\n",
    "    song_path,\n",
    "    rate,\n",
    "    audio_length,\n",
    "    L=2048,\n",
    "    hop=512, \n",
    "    display_audio = False,\n",
    "    display_room = False,\n",
    "    ):\n",
    "\n",
    "    \"\"\" \n",
    "    \n",
    "    this function process a custom recording of a song (NOT MUSDB18)\n",
    "    as a recording in a shoebox room (defined with its \n",
    "    geometry, room absorption, signal locations and \n",
    "    microphones locations) into a multichannel STFT.\n",
    "\n",
    "    Inputs:\n",
    "    ------------------------------------------------------\n",
    "    room_dimension: \n",
    "        room dimension (np.array)\n",
    "    abs_coef: \n",
    "        absorption coefficient (int)\n",
    "    source_locations: \n",
    "        localization of the sources in the room (list of list)\n",
    "    source_names: \n",
    "        name of the sources (list of string)\n",
    "    microphone_locations: \n",
    "        localization of the micros in the room (warning, locs in `np.c_` class)\n",
    "    microphone_names: \n",
    "        name of the microphones (list of string)\n",
    "    mic_dir:\n",
    "        microphone directivity (pyroomacoustics directivity object)\n",
    "    song_path: \n",
    "        path to the song (string)\n",
    "    rate:\n",
    "        sampling rate (int)\n",
    "    audio_length: \n",
    "        length of the audio signal (int)\n",
    "    L: \n",
    "        frame size (2048 default)\n",
    "    hop: \n",
    "        hop length (512 default)\n",
    "    display_audio: \n",
    "        bool (display audio signal)\n",
    "    display_room: \n",
    "        bool (display room geometry)\n",
    "\n",
    "    Output:\n",
    "    ------------------------------------------------------\n",
    "    X: (n_frames, n_frequencies, n_channels)\n",
    "    room: pyroomacoustics room object\n",
    "    separate_recordings: (n_sources, n_channels, n_frames)\n",
    "    mics_signals: (n_channels, n_frames)\n",
    "\n",
    "    \"\"\"\n",
    "    track_list = song_path\n",
    "\n",
    "    # Create an shoebox room\n",
    "    room = pra.ShoeBox(room_dimension, fs=rate, max_order=15, absorption=abs_coef, sigma2_awgn=1e-8)\n",
    "\n",
    "    # Add sources\n",
    "    for source, source_loc in zip(track_list, source_locations):\n",
    "        signal_channel = librosa.core.to_mono(source[:rate*audio_length, :].T)\n",
    "        signal_channel /= np.max(signal_channel)\n",
    "        room.add_source(source_loc, signal=signal_channel)\n",
    "\n",
    "    # Add microphone array\n",
    "    mic_array = pra.MicrophoneArray(microphone_locations, rate)\n",
    "    # Appoint => orienté dans la direction de l'instrument => cardioide pour l'instant\n",
    "    # Couple => ORTF 17cm, 110°, 90° angle utile\n",
    "    room.add_microphone_array(mic_array, directivity=mic_dir)\n",
    "    \n",
    "    if display_room:\n",
    "        fig, ax = room.plot()\n",
    "        lim = np.max(room_dimension)\n",
    "        ax.set_xlim([0, lim])\n",
    "        ax.set_ylim([0, lim])\n",
    "        ax.set_zlim([0, lim])\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        ax.set_zlabel('Z')\n",
    "    \n",
    "    # Recordings\n",
    "    separate_recordings = room.simulate(return_premix=True)\n",
    "    mics_signals = np.sum(separate_recordings, axis=0)\n",
    "    \n",
    "    # STFT parameters\n",
    "    win_a = pra.hamming(L)\n",
    "    win_s = pra.transform.stft.compute_synthesis_window(win_a, hop)\n",
    "          \n",
    "    # Observation vector in the STFT domain\n",
    "    X = pra.transform.stft.analysis(mics_signals.T, L, hop, win=win_a)\n",
    "\n",
    "    if display_audio:\n",
    "        for microphone_n in range(microphone_locations.shape[1]) :\n",
    "            print(f\"Microphone {microphone_names[microphone_n]}\")\n",
    "            display(ipd.Audio(mics_signals[microphone_n], rate=room.fs))\n",
    "\n",
    "    return X, room, separate_recordings, mics_signals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not MUSEDB:\n",
    "    print(\"working with custom signals\")\n",
    "    \n",
    "    # Parametres de la room\n",
    "    room_dimension = [12, 20, 5]\n",
    "    abs_coef = 0.35\n",
    "\n",
    "    # Parametres des sources: noms et localisations dans la salle\n",
    "    source_names = [\"source1\", \"source2\", \"source3\", \"source4\"]\n",
    "    source_locations = [[2, 9, 1], [2, 11, 1], [3, 12, 1], [3, 7, 1]]\n",
    "    \n",
    "    # Parametres des micros: noms et localisations dans la salle\n",
    "    microphone_names = [\"mic1\", \"mic2\", \"mic3\", \"mic4\", \"mic5\", \"mic6\"]\n",
    "    microphone_locations = np.c_[[2.7, 9, 1], [2.7, 11, 1], [3.5, 12, 1], [3.5, 7, 1], [6, 9.6, 1], [6, 10.4, 1]]\n",
    "\n",
    "    # Parametres de la STFT\n",
    "    L = 2048\n",
    "    hop = L // 4\n",
    "    \n",
    "    # Parametres de la musique\n",
    "    audio_length = 10\n",
    "    rate = 44100\n",
    "    \n",
    "    X, room, separate_recordings, mics_signals = spectrogram_from_wav(room_dimension, abs_coef, source_locations, source_names, microphone_locations, microphone_names, MIC_DIR, signals, rate=rate, audio_length=audio_length, L=L, hop=hop, display_audio=True, display_room=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jRNKWRlydbs7",
    "outputId": "9f32f76c-3b4c-4e3f-c188-58ed7d83cfb3"
   },
   "outputs": [],
   "source": [
    "if MUSEDB:\n",
    "    print(\"working with MUSDB18\")\n",
    "    \n",
    "    # Parametres de la room\n",
    "    room_dimension = [12, 20, 5]\n",
    "    abs_coef = 0.35\n",
    "\n",
    "    # Parametres des sources: noms et localisations dans la salle\n",
    "    # 1: Drums | 2: Bass | 3: Accompaniemenet | 4: Vocals\n",
    "    source_names = ['drums', 'bass', 'other', 'vocals']\n",
    "    source_locations = [[2, 9, 1], [2, 11, 1], [3, 12, 1], [3, 7, 1]]\n",
    "\n",
    "    # Parametres des micros: noms et localisations dans la salle\n",
    "    # 1: Drums | 2: Bass | 3: Accompaniemenet | 4: Vocals | 5 et 6 couples AB (espacés de 80cm)\n",
    "    microphone_names = ['drums', 'bass', 'other', 'vocals', 'AB1', 'AB2']\n",
    "    microphone_locations = np.c_[[2.7, 9, 1], [2.7, 11, 1], [3.5, 12, 1], [3.5, 7, 1], [6, 9.6, 1], [6, 10.4, 1]]\n",
    "    \n",
    "    # Prend une musique au hasard\n",
    "    ind = rd.randint(0, len(files_in))\n",
    "    song_path = files_in[2]\n",
    "\n",
    "    # Parametres de la STFT\n",
    "    L = 4096\n",
    "    hop = L // 4\n",
    "    \n",
    "    # Parametres de la musique\n",
    "    audio_length = 30\n",
    "\n",
    "    # Transformations des signaux audio en STFT_multichannel en prennant en compte la room et l'emplacement des micros et sources\n",
    "    X, room, separate_recordings, mics_signals = spectrogram_from_musdb(room_dimension, abs_coef, source_locations, source_names, microphone_locations, microphone_names, MIC_DIR, song_path, audio_length=audio_length, L=L, hop=hop, display_audio=True, display_room=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajout de la Fast MNMF pour la séparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FastMNMF2 from pyroomacoustics\n",
    "=========\n",
    "\n",
    "Blind Source Separation using Fast Multichannel Nonnegative Matrix Factorization 2 (FastMNMF2)\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def fastmnmf2(\n",
    "    X,\n",
    "    n_src=None,\n",
    "    n_iter=30,\n",
    "    n_components=32,\n",
    "    mic_index=0,\n",
    "    W0=None,\n",
    "    accelerate=True,\n",
    "    callback=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Implementation of FastMNMF2 algorithm presented in\n",
    "\n",
    "    K. Sekiguchi, Y. Bando, A. A. Nugraha, K. Yoshii, T. Kawahara, *Fast Multichannel Nonnegative\n",
    "    Matrix Factorization With Directivity-Aware Jointly-Diagonalizable Spatial\n",
    "    Covariance Matrices for Blind Source Separation*, IEEE/ACM TASLP, 2020.\n",
    "    [`IEEE <https://ieeexplore.ieee.org/abstract/document/9177266>`_]\n",
    "\n",
    "    The code of FastMNMF2 with GPU support and more sophisticated initialization\n",
    "    is available on  https://github.com/sekiguchi92/SoundSourceSeparation\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X: ndarray (nframes, nfrequencies, nchannels)\n",
    "        STFT representation of the observed signal\n",
    "    n_src: int, optional\n",
    "        The number of sound sources (default None).\n",
    "        If None, n_src is set to the number of microphones\n",
    "    n_iter: int, optional\n",
    "        The number of iterations (default 30)\n",
    "    n_components: int, optional\n",
    "        Number of components in the non-negative spectrum (default 8)\n",
    "    mic_index: int or 'all', optional\n",
    "        The index of microphone of which you want to get the source image (default 0).\n",
    "        If 'all', return the source images of all microphones\n",
    "    W0: ndarray (nfrequencies, nchannels, nchannels), optional\n",
    "        Initial value for diagonalizer Q (default None).\n",
    "        If None, identity matrices are used for all frequency bins.\n",
    "    accelerate: bool, optional\n",
    "        If true, the basis and activation of NMF are updated simultaneously (default True)\n",
    "    callback: func, optional\n",
    "        A callback function called every 10 iterations, allows to monitor convergence\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    If mic_index is int, returns an (nframes, nfrequencies, nsources) array.\n",
    "    If mic_index is 'all', returns an (nchannels, nframes, nfrequencies, nsources) array.\n",
    "    \"\"\"\n",
    "    eps = 1e-10\n",
    "    g_eps = 5e-2\n",
    "    interval_update_Q = 1  # 2 may work as well and is faster\n",
    "    interval_normalize = 10\n",
    "    TYPE_FLOAT = X.real.dtype\n",
    "    TYPE_COMPLEX = X.dtype\n",
    "\n",
    "    # initialize parameter\n",
    "    X_FTM = X.transpose(1, 0, 2)\n",
    "    n_freq, n_frames, n_chan = X_FTM.shape\n",
    "    XX_FTMM = np.matmul(X_FTM[:, :, :, None], X_FTM[:, :, None, :].conj())\n",
    "    if n_src is None:\n",
    "        n_src = X_FTM.shape[2]\n",
    "\n",
    "    if W0 is not None:\n",
    "        Q_FMM = W0\n",
    "    else:\n",
    "        Q_FMM = np.tile(np.eye(n_chan).astype(TYPE_COMPLEX), [n_freq, 1, 1])\n",
    "\n",
    "    g_NM = np.ones([n_src, n_chan], dtype=TYPE_FLOAT) * g_eps\n",
    "    for m in range(n_chan):\n",
    "        g_NM[m % n_src, m] = 1\n",
    "\n",
    "    for m in range(n_chan):\n",
    "        mu_F = (Q_FMM[:, m] * Q_FMM[:, m].conj()).sum(axis=1).real\n",
    "        Q_FMM[:, m] /= np.sqrt(mu_F[:, None])\n",
    "\n",
    "    H_NKT = np.random.rand(n_src, n_components, n_frames).astype(TYPE_FLOAT)\n",
    "    W_NFK = np.random.rand(n_src, n_freq, n_components).astype(TYPE_FLOAT)\n",
    "    lambda_NFT = W_NFK @ H_NKT\n",
    "    Qx_power_FTM = np.abs(np.einsum(\"fij, ftj -> fti\", Q_FMM, X_FTM)) ** 2\n",
    "    Y_FTM = np.einsum(\"nft, nm -> ftm\", lambda_NFT, g_NM)\n",
    "\n",
    "    def separate():\n",
    "        Qx_FTM = np.einsum(\"fij, ftj -> fti\", Q_FMM, X_FTM)\n",
    "        Qinv_FMM = np.linalg.inv(Q_FMM)\n",
    "        Y_NFTM = np.einsum(\"nft, nm -> nftm\", lambda_NFT, g_NM)\n",
    "\n",
    "        if mic_index == \"all\":\n",
    "            return np.einsum(\n",
    "                \"fij, ftj, nftj -> itfn\", Qinv_FMM, Qx_FTM / Y_NFTM.sum(axis=0), Y_NFTM\n",
    "            )\n",
    "        elif type(mic_index) is int:\n",
    "            return np.einsum(\n",
    "                \"fj, ftj, nftj -> tfn\",\n",
    "                Qinv_FMM[:, mic_index],\n",
    "                Qx_FTM / Y_NFTM.sum(axis=0),\n",
    "                Y_NFTM,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\"mic_index should be int or 'all'\")\n",
    "\n",
    "    # update parameters\n",
    "    for epoch in range(n_iter):\n",
    "        if callback is not None and epoch % 10 == 0:\n",
    "            callback(separate())\n",
    "\n",
    "        # update W and H (basis and activation of NMF)\n",
    "        tmp1_NFT = np.einsum(\"nm, ftm -> nft\", g_NM, Qx_power_FTM / (Y_FTM**2))\n",
    "        tmp2_NFT = np.einsum(\"nm, ftm -> nft\", g_NM, 1 / Y_FTM)\n",
    "\n",
    "        numerator = np.einsum(\"nkt, nft -> nfk\", H_NKT, tmp1_NFT)\n",
    "        denominator = np.einsum(\"nkt, nft -> nfk\", H_NKT, tmp2_NFT)\n",
    "        W_NFK *= np.sqrt(numerator / denominator)\n",
    "\n",
    "        if not accelerate:\n",
    "            tmp1_NFT = np.einsum(\"nm, ftm -> nft\", g_NM, Qx_power_FTM / (Y_FTM**2))\n",
    "            tmp2_NFT = np.einsum(\"nm, ftm -> nft\", g_NM, 1 / Y_FTM)\n",
    "            lambda_NFT = W_NFK @ H_NKT + eps\n",
    "            Y_FTM = np.einsum(\"nft, nm -> ftm\", lambda_NFT, g_NM) + eps\n",
    "\n",
    "        numerator = np.einsum(\"nfk, nft -> nkt\", W_NFK, tmp1_NFT)\n",
    "        denominator = np.einsum(\"nfk, nft -> nkt\", W_NFK, tmp2_NFT)\n",
    "        H_NKT *= np.sqrt(numerator / denominator)\n",
    "\n",
    "        lambda_NFT = W_NFK @ H_NKT + eps\n",
    "        Y_FTM = np.einsum(\"nft, nm -> ftm\", lambda_NFT, g_NM) + eps\n",
    "\n",
    "        # update g_NM (diagonal element of spatial covariance matrices)\n",
    "        numerator = np.einsum(\"nft, ftm -> nm\", lambda_NFT, Qx_power_FTM / (Y_FTM**2))\n",
    "        denominator = np.einsum(\"nft, ftm -> nm\", lambda_NFT, 1 / Y_FTM)\n",
    "        g_NM *= np.sqrt(numerator / denominator)\n",
    "        Y_FTM = np.einsum(\"nft, nm -> ftm\", lambda_NFT, g_NM) + eps\n",
    "\n",
    "        # udpate Q (joint diagonalizer)\n",
    "        if (interval_update_Q <= 0) or (epoch % interval_update_Q == 0):\n",
    "            for m in range(n_chan):\n",
    "                V_FMM = (\n",
    "                    np.einsum(\"ftij, ft -> fij\", XX_FTMM, 1 / Y_FTM[..., m]) / n_frames\n",
    "                )\n",
    "                tmp_FM = np.linalg.solve(\n",
    "                    np.matmul(Q_FMM, V_FMM), np.eye(n_chan)[None, m]\n",
    "                )\n",
    "                Q_FMM[:, m] = (\n",
    "                    tmp_FM\n",
    "                    / np.sqrt(\n",
    "                        np.einsum(\"fi, fij, fj -> f\", tmp_FM.conj(), V_FMM, tmp_FM)\n",
    "                    )[:, None]  \n",
    "                ).conj()\n",
    "                Qx_power_FTM = np.abs(np.einsum(\"fij, ftj -> fti\", Q_FMM, X_FTM)) ** 2\n",
    "\n",
    "        # normalize\n",
    "        if (interval_normalize <= 0) or (epoch % interval_normalize == 0):\n",
    "            phi_F = np.einsum(\"fij, fij -> f\", Q_FMM, Q_FMM.conj()).real / n_chan\n",
    "            Q_FMM /= np.sqrt(phi_F)[:, None, None]\n",
    "            W_NFK /= phi_F[None, :, None]\n",
    "\n",
    "            mu_N = g_NM.sum(axis=1)\n",
    "            g_NM /= mu_N[:, None]\n",
    "            W_NFK *= mu_N[:, None, None]\n",
    "\n",
    "            nu_NK = W_NFK.sum(axis=1)\n",
    "            W_NFK /= nu_NK[:, None]\n",
    "            H_NKT *= nu_NK[:, :, None]\n",
    "\n",
    "            lambda_NFT = W_NFK @ H_NKT + eps\n",
    "            Qx_power_FTM = np.abs(np.einsum(\"fij, ftj -> fti\", Q_FMM, X_FTM)) ** 2\n",
    "            Y_FTM = np.einsum(\"nft, nm -> ftm\", lambda_NFT, g_NM) + eps\n",
    "\n",
    "    return separate(), W_NFK, H_NKT, Y_FTM, g_NM, Q_FMM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shape_separate_recordings=(n_sources, n_mics, n_samples)\n",
    "# ref0 = separate_recordings[:, 0, :]\n",
    "SDR, SIR, SAR, PERM = [], [], [], []\n",
    "\n",
    "win_a = pra.hamming(L)\n",
    "win_s = pra.transform.stft.compute_synthesis_window(win_a, hop)\n",
    "\n",
    "# Callback function to monitor the convergence of the algorithm\n",
    "def convergence_callback_micro(Y):\n",
    "    global SDR, SIR, SAR, PERM\n",
    "    sdr = np.zeros(len(Y), dtype=object)\n",
    "    sir = np.zeros(len(Y), dtype=object)\n",
    "    sar = np.zeros(len(Y), dtype=object)\n",
    "    perm = np.zeros(len(Y), dtype=object)\n",
    "    \n",
    "    for i in range(len(Y)):\n",
    "        signal_ = pra.transform.stft.synthesis(Y[i], L, hop, win=win_s)\n",
    "        signal_ = signal_[L - hop:, :].T\n",
    "        \n",
    "        # shape_separate_recordings=(n_sources, n_mics, n_samples)\n",
    "        ref_ = separate_recordings[:, i, :]\n",
    "        \n",
    "        m_ = np.minimum(signal_.shape[1], ref_.shape[1])\n",
    "        \n",
    "        sdr[i], sir[i], sar[i], perm[i] = bss_eval_sources(ref_[:,:m_], signal_[:,:m_])\n",
    "        \n",
    "    SDR.append(sdr)\n",
    "    SIR.append(sir)\n",
    "    SAR.append(sar)\n",
    "    PERM.append(perm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parametres de l'algorithme\n",
    "n_basis = 32\n",
    "n_iter = 100\n",
    "    \n",
    "if PLOT_PERFORMANCE:\n",
    "    print(\"Running fastmnmf2 with callback function\")\n",
    "    Y, W_NFK, H_NKT, Y_FTM, g_NM, Q_FMM = fastmnmf2(X, n_src=len(source_names), n_iter=n_iter, n_components=n_basis, mic_index='all', W0=None, accelerate=True, callback=convergence_callback_micro)\n",
    "\n",
    "elif LOUIS:\n",
    "    print(\"Running fastmnmf2 with Louis custom algotithm\")\n",
    "    Y, W_NFK, H_NKT, g_NM, Q_FMM, Qx_FTM, X_tilde_FTM, Y_tilde_FTM = fast_MNMF2(X.transpose(1,0,2), n_iter=n_iter, n_microphones=len(microphone_names), n_sources=len(source_names), n_time_frames=X.shape[0], n_freq_bins=X.shape[1], n_basis=n_basis, algo='IP', mic_index=None)\n",
    "    Y = Y.transpose(0,3,2,1)\n",
    "    \n",
    "elif not PLOT_PERFORMANCE and not LOUIS:\n",
    "    print(\"Running fastmnmf2 without callback function\")\n",
    "    Y, W_NFK, H_NKT, Y_FTM, g_NM, Q_FMM = fastmnmf2(X, n_src=len(source_names), n_iter=n_iter, n_components=n_basis, mic_index='all', W0=None, accelerate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STFT parameters\n",
    "win_a = pra.hamming(L)\n",
    "win_s = pra.transform.stft.compute_synthesis_window(win_a, hop)\n",
    "\n",
    "y = []\n",
    "ref = []\n",
    "m = []\n",
    "\n",
    "sdr = []\n",
    "si_sdr = []\n",
    "sir = []\n",
    "sar = []\n",
    "perm = []\n",
    "\n",
    "for i in range(len(Y)):\n",
    "        \n",
    "    signal_ = pra.transform.stft.synthesis(Y[i], L, hop, win=win_s)\n",
    "    signal_ = signal_[L - hop:, :].T\n",
    "    y.append(signal_)\n",
    "    \n",
    "    ref_ = separate_recordings[:, i, :]\n",
    "    ref.append(ref_)\n",
    "    \n",
    "    m_ = np.minimum(signal_.shape[1], ref_.shape[1])\n",
    "    m.append(m_)\n",
    "    \n",
    "    sdr_, sir_, sar_, perm_ = bss_eval_sources(ref_[:,:m_], signal_[:,:m_])\n",
    "    si_sdr_ = performance.si_sdr(ref_[:,:m_], signal_[:,:m_])\n",
    "    \n",
    "    sdr.append(sdr_)\n",
    "    si_sdr.append(si_sdr_)\n",
    "    sir.append(sir_)\n",
    "    sar.append(sar_)\n",
    "    perm.append(perm_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print des sdr et sir finaux\n",
    "print(\"SDR final : \", np.mean(sdr))\n",
    "print(\"SI_SDR final : \", np.mean(si_sdr))\n",
    "print(\"SIR final : \", np.mean(sir))\n",
    "print(\"SAR final : \", np.mean(sar))\n",
    "\n",
    "perf_mean = np.array([{\"sdr_mean\": np.mean(sdr)}, {\"si_sdr_mean\": np.mean(si_sdr)}, {\"sir_mean\": np.mean(sir)}, {\"sar_mean\": np.mean(sar)}], dtype=object)\n",
    "perf_final = np.array([{\"sdr_final\": sdr}, {\"si_sdr_final\": si_sdr},{\"sir_final\": sir}, {\"sar_final\": sar}], dtype=object)\n",
    "\n",
    "perf = np.concatenate((perf_mean, perf_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if SAVE_PERF:\n",
    "    # save sdr, sir, sar in a csv file \n",
    "    np.savetxt(save_path+\"perf_\"+str(song_path.split(\"/\")[2][:-9])+\"_audio_length_\"+str(audio_length)+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".csv\", perf, delimiter=\",\", fmt=\"%s\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Un peu de visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspond à la séparation de source, micro_plot_sep\n",
    "mic_plot_sep = 5\n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 6)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.specgram(ref[mic_plot_sep][0,:], NFFT=1024, Fs=room.fs)\n",
    "plt.title('Source 0 (target)')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.specgram(ref[mic_plot_sep][1,:], NFFT=1024, Fs=room.fs)\n",
    "plt.title('Source 1 (target)')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.specgram(y[mic_plot_sep][0,:], NFFT=1024, Fs=room.fs)\n",
    "plt.title('Source 0 (séparé)')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.specgram(y[mic_plot_sep][1,:], NFFT=1024, Fs=room.fs)\n",
    "plt.title('Source 1 (séparé)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.tight_layout(pad=2.5)\n",
    "fig.suptitle(\"spectro_source01_micro_\"+microphone_names[mic_plot_sep]+str(song_path.split(\"/\")[2][:-9])+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L))\n",
    "\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(save_path+\"spectro/\"+\"spectro_source01_micro_\"+microphone_names[mic_plot_sep]+str(song_path.split(\"/\")[2][:-9])+\"_audio_length_\"+str(audio_length)+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspond à la séparation de source, micro_plot_sep \n",
    "\n",
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 6)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.specgram(ref[mic_plot_sep][2,:], NFFT=1024, Fs=room.fs)\n",
    "plt.title('Source 2 (clean)')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.specgram(ref[mic_plot_sep][3,:], NFFT=1024, Fs=room.fs)\n",
    "plt.title('Source 3 (clean)')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.specgram(y[mic_plot_sep][2,:], NFFT=1024, Fs=room.fs)\n",
    "plt.title('Source 2 (separated)')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.specgram(y[mic_plot_sep][3,:], NFFT=1024, Fs=room.fs)\n",
    "plt.title('Source 3 (separated)')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.tight_layout(pad=2.5)\n",
    "fig.suptitle(\"spectro_source23_micro_\"+microphone_names[mic_plot_sep]+str(song_path.split(\"/\")[2][:-9])+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L))\n",
    "\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(save_path+\"spectro/\"+\"spectro_source23_micro_\"+microphone_names[mic_plot_sep]+str(song_path.split(\"/\")[2][:-9])+\"_audio_length_\"+str(audio_length)+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correspond à la séparation de source, micro_plot_sep \n",
    "\n",
    "if PLOT_PERFORMANCE: \n",
    "    \n",
    "    print(\"Plotting performance for fastmnmf2\")\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(10, 6)\n",
    "    a = np.array(SDR)\n",
    "    a = np.stack(a[:, mic_plot_perf])\n",
    "\n",
    "    b = np.array(SIR)\n",
    "    b = np.stack(b[:, mic_plot_perf])\n",
    "\n",
    "    plt.plot(np.arange(a.shape[0]) * 10, a[:,0], label='SDR Source 0', c='r', ls='-')\n",
    "    plt.plot(np.arange(a.shape[0]) * 10, a[:,1], label='SDR Source 1', c='r', ls='--')\n",
    "    plt.plot(np.arange(a.shape[0]) * 10, a[:,2], label='SDR Source 2', c='r', ls=':')\n",
    "    plt.plot(np.arange(a.shape[0]) * 10, a[:,3], label='SDR Source 3', c='r', ls='-.')\n",
    "\n",
    "    plt.plot(np.arange(b.shape[0]) * 10, b[:,0], label='SIR Source 0', c='b', ls='-')\n",
    "    plt.plot(np.arange(b.shape[0]) * 10, b[:,1], label='SIR Source 1', c='b', ls='--')\n",
    "    plt.plot(np.arange(b.shape[0]) * 10, b[:,2], label='SIR Source 2', c='b', ls=':')\n",
    "    plt.plot(np.arange(b.shape[0]) * 10, b[:,3], label='SIR Source 3', c='b', ls='-.')\n",
    "\n",
    "    plt.legend(ncol=1)\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('dB')\n",
    "    plt.grid()\n",
    "    plt.title('performance microphone 0')\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ecoute maintenant "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Pas de séparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Listening of Audio at each microphones without separation\")\n",
    "\n",
    "for micro_n in range(len(mics_signals)):\n",
    "    print('microphone ', microphone_names[micro_n])\n",
    "    display(ipd.Audio(mics_signals[micro_n], rate=44100))\n",
    "    if SAVE_AUDIO:\n",
    "        wavfile.write(save_path+\"audios/no_separation/micro_\"+microphone_names[micro_n]+\"/\"+str(song_path.split(\"/\")[2][:-9])+\"_globals_micro_\"+microphone_names[micro_n]+\".wav\", 44100, mics_signals[micro_n]) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Séparation des sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ecoute_separation_micro(mic, y, save):\n",
    "    \"\"\" Fonction permettant d'écouter les audios séparés pour un micro donné et de les sauvegarder si save=True\n",
    "\n",
    "    Args:\n",
    "        mic (int): index du microphone dont on veut écouter la séparation\n",
    "        y (array): array contenant les signaux audios séparés pour chaque microphones\n",
    "        save (boolean): True si on veut sauvegarder les audios séparés, False sinon\n",
    "    \"\"\"\n",
    "\n",
    "    for source_n in range(len(y[0])):\n",
    "        if source_n == 0:\n",
    "            print(\"Drums séparé microphone \", microphone_names[mic])\n",
    "            display(ipd.Audio(y[mic][source_n], rate=44100))\n",
    "            if save:\n",
    "                wavfile.write(save_path+\"audios/separation/micro_\"+microphone_names[mic]+\"/\"+str(song_path.split(\"/\")[2][:-9])+\"_drums_micro_\"+microphone_names[mic]+\"_audio_length_\"+str(audio_length)+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".wav\", 44100, y[mic][source_n]) \n",
    "            \n",
    "        elif source_n == 1:\n",
    "            print(\"Bass séparé microphone \", microphone_names[mic])\n",
    "            display(ipd.Audio(y[mic][source_n], rate=44100))\n",
    "            if save:\n",
    "                wavfile.write(save_path+\"audios/separation/micro_\"+microphone_names[mic]+\"/\"+str(song_path.split(\"/\")[2][:-9])+\"_bass_micro_\"+microphone_names[mic]+\"_audio_length_\"+str(audio_length)+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".wav\", 44100, y[mic][source_n]) \n",
    "        \n",
    "        elif source_n == 2:\n",
    "            print(\"Accompaniement séparé microphone \", microphone_names[mic])\n",
    "            display(ipd.Audio(y[mic][source_n], rate=44100))\n",
    "            if save:\n",
    "                wavfile.write(save_path+\"audios/separation/micro_\"+microphone_names[mic]+\"/\"+str(song_path.split(\"/\")[2][:-9])+\"_other_micro_\"+microphone_names[mic]+\"_audio_length_\"+str(audio_length)+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".wav\", 44100, y[mic][source_n]) \n",
    "        \n",
    "        elif source_n == 3:\n",
    "            print(\"Vocals séparé microphone \", microphone_names[mic])\n",
    "            display(ipd.Audio(y[mic][source_n], rate=44100))\n",
    "            if save:\n",
    "                wavfile.write(save_path+\"audios/separation/micro_\"+microphone_names[mic]+\"/\"+str(song_path.split(\"/\")[2][:-9])+\"_vocals_micro_\"+microphone_names[mic]+\"_audio_length_\"+str(audio_length)+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".wav\", 44100, y[mic][source_n]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Listening of Audios at microphones 0 (couple) with separation\")\n",
    "ecoute_separation_micro(0, y, SAVE_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Listening of Audios at microphones 1 (couple) with separation\")\n",
    "ecoute_separation_micro(1, y, SAVE_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Listening of Audios at microphones 2 (couple) with separation\")\n",
    "ecoute_separation_micro(2, y, SAVE_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Listening of Audios at microphones 3 (couple) with separation\")\n",
    "ecoute_separation_micro(3, y, SAVE_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Listening of Audios at microphones 4 (couple) with separation\")\n",
    "ecoute_separation_micro(4, y, SAVE_AUDIO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Listening of Audios at microphones 5 (couple) with separation\")\n",
    "ecoute_separation_micro(5, y, SAVE_AUDIO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualisation des matrices intermédiaires"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Représentation of g, and G (covariance et matrice spatiale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not LOUIS:\n",
    "    plt.imshow(g_NM[:, :], cmap='inferno', aspect='auto')\n",
    "    plt.title('g_NM')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    if SAVE_FIG:\n",
    "        plt.savefig(save_path+\"mix/\"+\"g_mn_\"+str(song_path.split(\"/\")[2])+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".pdf\")   \n",
    "    \n",
    "    \n",
    "elif LOUIS:\n",
    "    plt.imshow(g_NM[:, :], cmap='inferno', aspect='auto')\n",
    "    plt.title(\"g_mn_\"+str(song_path.split(\"/\")[2][:-9])+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if SAVE_FIG:\n",
    "        plt.savefig(save_path+\"mix/\"+\"g_mn_\"+str(song_path.split(\"/\")[2][:-9])+\"_audio_length_\"+str(audio_length)+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_mix(Q_FMM, g_NM):\n",
    "    g_NMM = []\n",
    "    for n_source in range(g_NM.shape[0]):\n",
    "        g_NMM.append(np.diag(g_NM[n_source]))\n",
    "\n",
    "    g_NMM = np.array(g_NMM)\n",
    "    \n",
    "    G_NF = np.einsum(\"fij, nij, ijf -> ni\", np.linalg.inv(Q_FMM), g_NMM, np.linalg.inv(Q_FMM).conj().T)\n",
    "    return G_NF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G_NF\n",
    "G_NF = G_mix(Q_FMM, g_NM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(np.log(np.abs(G_NF)), cmap='inferno', aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title(\"G_NF_\"+str(song_path.split(\"/\")[2][:-9])+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L))\n",
    "plt.tight_layout()\n",
    "\n",
    "if SAVE_FIG:\n",
    "    plt.savefig(save_path+\"mix/\"+\"G_NF_\"+str(song_path.split(\"/\")[2][:-9])+\"_audio_length_\"+str(audio_length)+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".pdf\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Représentation de W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 6)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(np.log(W_NFK[0, :, :]), cmap='inferno', aspect='auto')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Matrice de base W0')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(np.log(W_NFK[1, :, :]), cmap='inferno', aspect='auto')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Matrice de base W1')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(np.log(W_NFK[2, :, :]), cmap='inferno', aspect='auto')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Matrice de base W2')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(np.log(W_NFK[3, :, :]), cmap='inferno', aspect='auto')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Matrice de base W3')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.tight_layout(pad=2.5)\n",
    "fig.suptitle(\"W_NFK_\"+str(song_path.split(\"/\")[2][:-9])+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L))\n",
    "\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(save_path+\"base/\"+\"W_NFK_\"+str(song_path.split(\"/\")[2][:-9])+\"_audio_length_\"+str(audio_length)+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".pdf\") "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Représentation des activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "fig.set_size_inches(10, 6)\n",
    "\n",
    "plt.subplot(2,2,1)\n",
    "plt.imshow(H_NKT[0, :, :], cmap='inferno', aspect='auto')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Matrice de base H0')\n",
    "\n",
    "plt.subplot(2,2,2)\n",
    "plt.imshow(H_NKT[1, :, :], cmap='inferno', aspect='auto')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Matrice de base H1')\n",
    "\n",
    "plt.subplot(2,2,3)\n",
    "plt.imshow(H_NKT[2, :, :], cmap='inferno', aspect='auto')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Matrice de base H2')\n",
    "\n",
    "plt.subplot(2,2,4)\n",
    "plt.imshow(H_NKT[3, :, :], cmap='inferno', aspect='auto')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.title('Matrice de base H3')\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "fig.tight_layout(pad=2.5)\n",
    "fig.suptitle(\"H_NKT_\"+str(song_path.split(\"/\")[2][:-9])+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L))\n",
    "\n",
    "if SAVE_FIG:\n",
    "    fig.savefig(save_path+\"activation/\"+\"H_NKT_\"+str(song_path.split(\"/\")[2][:-9])+\"_audio_length_\"+str(audio_length)+\"_n_basis_\"+str(n_basis)+\"_n_fft_\"+str(L)+\".pdf\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "P3S",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8 (main, Nov 24 2022, 14:13:03) [GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2011a595c94b6a73b54060f6a4d9915a152f934cd74fcad2403d0e39c89de16a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
